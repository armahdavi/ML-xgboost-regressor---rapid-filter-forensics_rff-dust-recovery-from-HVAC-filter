{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOcanKA8jHthxgsCzk3wloy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/armahdavi/ML-xgboost-regressor---rapid-filter-forensics_rff-dust-recovery-from-HVAC-filter/blob/main/lt_package_pyspark_practive.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8gQVRFIGSG1",
        "outputId": "bb59d91a-2a24-4d73-9a8a-f0c7b1b36608"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488490 sha256=ad08113baa022ca181f06d53ecf6562fdcbe7168a54bd07177b7a68546625e44\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ],
      "source": [
        "# Install PySpark\n",
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup PySpark in Google Colab\n",
        "from pyspark.sql import SparkSession\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrhikQi-O9h9",
        "outputId": "8246ac97-1b18-43d7-d8d2-aeef514e4ddb"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the file path in your Google Drive\n",
        "file_path = '/content/drive/My Drive/lt_complete_weekly_sum.csv'\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "# Read the CSV file into a PySpark DataFrame\n",
        "df = spark.read.csv(file_path, header=True)\n",
        "col_list = df.columns\n",
        "\n",
        "df = df.withColumn(\"v\", lit(0.1))\n",
        "df = df.withColumn(\"met\", lit(1.0))\n",
        "df = df.withColumn(\"clo\", lit(0.5))\n",
        "\n",
        "\n",
        "# selected_cols = ['GMTtime'] + col_list[col_list.index('tmp'):col_list.index('PPD_10')]\n",
        "df = df.select('tmp', 'mrt', 'v', 'rhm', 'met', 'clo')\n",
        "print(type(df))\n",
        "df.show(n = 100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dlaPKLRPI0D",
        "outputId": "69e86799-510a-4f5a-d773-7b53111b171d"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pyspark.sql.dataframe.DataFrame'>\n",
            "+------+------+---+------+---+---+\n",
            "|   tmp|   mrt|  v|   rhm|met|clo|\n",
            "+------+------+---+------+---+---+\n",
            "|28.742|27.924|0.1|17.576|1.0|0.5|\n",
            "|28.667| 27.85|0.1|17.219|1.0|0.5|\n",
            "|28.642|27.899|0.1|17.147|1.0|0.5|\n",
            "|28.642|27.875|0.1|16.864|1.0|0.5|\n",
            "|28.642|27.825|0.1|16.476|1.0|0.5|\n",
            "|28.593|27.825|0.1|16.225|1.0|0.5|\n",
            "|28.543|27.751|0.1|16.152|1.0|0.5|\n",
            "|28.518|27.727|0.1|16.222|1.0|0.5|\n",
            "|28.493|27.751|0.1| 16.22|1.0|0.5|\n",
            "|28.468|27.653|0.1|15.865|1.0|0.5|\n",
            "|28.444|27.628|0.1|15.723|1.0|0.5|\n",
            "|28.419|27.628|0.1|15.651|1.0|0.5|\n",
            "|28.369|27.604|0.1|15.577|1.0|0.5|\n",
            "|28.345| 27.53|0.1|15.576|1.0|0.5|\n",
            "| 28.32|27.554|0.1|15.504|1.0|0.5|\n",
            "| 28.32|27.554|0.1|15.504|1.0|0.5|\n",
            "|28.295|27.481|0.1|15.148|1.0|0.5|\n",
            "|28.245|27.431|0.1|15.146|1.0|0.5|\n",
            "|28.221|27.456|0.1|15.216|1.0|0.5|\n",
            "|28.171|27.382|0.1|15.284|1.0|0.5|\n",
            "|28.171|27.407|0.1|15.249|1.0|0.5|\n",
            "|28.171|27.382|0.1|15.249|1.0|0.5|\n",
            "|28.147|27.308|0.1|14.929|1.0|0.5|\n",
            "|28.072|27.259|0.1|15.634|1.0|0.5|\n",
            "|27.998|27.186|0.1|15.701|1.0|0.5|\n",
            "|27.974|27.186|0.1|15.346|1.0|0.5|\n",
            "|27.998|27.235|0.1|15.205|1.0|0.5|\n",
            "|28.023|27.308|0.1|15.065|1.0|0.5|\n",
            "|28.072|27.382|0.1|15.067|1.0|0.5|\n",
            "|28.122|27.456|0.1|15.069|1.0|0.5|\n",
            "|28.171|27.505|0.1|15.001|1.0|0.5|\n",
            "| 28.27|27.653|0.1|15.005|1.0|0.5|\n",
            "|28.419| 27.85|0.1|15.012|1.0|0.5|\n",
            "|28.568|27.998|0.1|14.665|1.0|0.5|\n",
            "|28.667|28.122|0.1|14.669|1.0|0.5|\n",
            "|28.766|28.196|0.1|15.313|1.0|0.5|\n",
            "|28.941|28.444|0.1|15.463|1.0|0.5|\n",
            "|29.115|28.593|0.1|14.832|1.0|0.5|\n",
            "|29.265|28.692|0.1|14.555|1.0|0.5|\n",
            "|29.389|28.766|0.1|14.347|1.0|0.5|\n",
            "|29.464|28.841|0.1|14.279|1.0|0.5|\n",
            "| 29.59|28.965|0.1|13.928|1.0|0.5|\n",
            "| 29.74|29.165|0.1|13.507|1.0|0.5|\n",
            "|29.916| 29.34|0.1|13.229|1.0|0.5|\n",
            "|30.016| 29.34|0.1|12.517|1.0|0.5|\n",
            "|30.066|29.365|0.1|12.232|1.0|0.5|\n",
            "|30.066|29.414|0.1|11.873|1.0|0.5|\n",
            "|30.066|29.389|0.1|11.622|1.0|0.5|\n",
            "|30.117|29.665|0.1|13.988|1.0|0.5|\n",
            "|30.142|29.464|0.1|14.168|1.0|0.5|\n",
            "|30.192|29.439|0.1|13.026|1.0|0.5|\n",
            "|30.167|29.389|0.1|12.452|1.0|0.5|\n",
            "|30.091|29.315|0.1|12.054|1.0|0.5|\n",
            "|30.016|29.215|0.1|11.728|1.0|0.5|\n",
            "|29.966|29.215|0.1|11.726|1.0|0.5|\n",
            "|29.916|29.115|0.1|11.508|1.0|0.5|\n",
            "| 29.84|28.941|0.1|11.038|1.0|0.5|\n",
            "|29.715|28.841|0.1|10.854|1.0|0.5|\n",
            "|29.565|28.642|0.1|10.812|1.0|0.5|\n",
            "|29.464|28.617|0.1| 10.88|1.0|0.5|\n",
            "|29.389|28.543|0.1| 10.77|1.0|0.5|\n",
            "| 29.29|28.419|0.1|10.694|1.0|0.5|\n",
            "|29.165| 28.27|0.1|11.622|1.0|0.5|\n",
            "|29.065|28.196|0.1|11.905|1.0|0.5|\n",
            "| 28.99|28.171|0.1|12.367|1.0|0.5|\n",
            "| 28.99|28.419|0.1|14.329|1.0|0.5|\n",
            "| 29.09|28.518|0.1|16.746|1.0|0.5|\n",
            "|29.165|28.568|0.1|17.739|1.0|0.5|\n",
            "|29.165|28.419|0.1|18.867|1.0|0.5|\n",
            "| 29.14|28.394|0.1|19.709|1.0|0.5|\n",
            "|29.115|28.295|0.1|18.723|1.0|0.5|\n",
            "| 29.04|28.196|0.1|16.956|1.0|0.5|\n",
            "|28.965|28.023|0.1| 15.89|1.0|0.5|\n",
            "|28.841|27.924|0.1|  15.6|1.0|0.5|\n",
            "|28.791|27.998|0.1|15.314|1.0|0.5|\n",
            "|28.791|27.998|0.1|15.101|1.0|0.5|\n",
            "|28.766|27.974|0.1|14.674|1.0|0.5|\n",
            "|28.742|27.924|0.1|14.317|1.0|0.5|\n",
            "|28.717| 27.85|0.1|14.174|1.0|0.5|\n",
            "|28.617|27.702|0.1|14.027|1.0|0.5|\n",
            "|28.518|27.628|0.1|13.809|1.0|0.5|\n",
            "|28.468|27.604|0.1|13.664|1.0|0.5|\n",
            "|28.444|27.677|0.1|13.663|1.0|0.5|\n",
            "|28.493|27.751|0.1| 13.63|1.0|0.5|\n",
            "|28.518|27.727|0.1|13.595|1.0|0.5|\n",
            "|28.493|27.702|0.1|13.452|1.0|0.5|\n",
            "|28.468|27.727|0.1|13.451|1.0|0.5|\n",
            "|28.419|27.702|0.1| 13.52|1.0|0.5|\n",
            "|28.419|27.751|0.1|13.591|1.0|0.5|\n",
            "|28.568|27.974|0.1|13.526|1.0|0.5|\n",
            "|28.642|27.998|0.1|13.815|1.0|0.5|\n",
            "|28.692|28.023|0.1|14.528|1.0|0.5|\n",
            "|28.692|27.949|0.1|17.079|1.0|0.5|\n",
            "|28.717|28.023|0.1|15.736|1.0|0.5|\n",
            "|28.717|28.023|0.1| 15.63|1.0|0.5|\n",
            "|28.766|28.097|0.1| 15.49|1.0|0.5|\n",
            "|28.791|28.122|0.1|15.314|1.0|0.5|\n",
            "|28.791|28.023|0.1|15.172|1.0|0.5|\n",
            "|28.742|27.949|0.1| 15.17|1.0|0.5|\n",
            "|28.692|27.949|0.1|15.167|1.0|0.5|\n",
            "+------+------+---+------+---+---+\n",
            "only showing top 100 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pythermalcomfort"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZA3qlLLTTZ2",
        "outputId": "4fc965a8-d4e3-4858-9f5e-406ef4fff6f1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pythermalcomfort\n",
            "  Downloading pythermalcomfort-2.10.0-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pythermalcomfort) (1.13.1)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from pythermalcomfort) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pythermalcomfort) (1.26.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from pythermalcomfort) (71.0.4)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->pythermalcomfort) (0.43.0)\n",
            "Downloading pythermalcomfort-2.10.0-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/128.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.0/128.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pythermalcomfort\n",
            "Successfully installed pythermalcomfort-2.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StructType, StructField, FloatType\n",
        "import pythermalcomfort as pt\n",
        "from pyspark.sql.functions import lit\n",
        "\n",
        "# Define the UDF to calculate PMV and PPD\n",
        "def calculate_pmv_ppd(ta, tr, v, rh, met, clo):\n",
        "    pmv, ppd = pt.pmv_ppd(ta, tr, v, rh, met, clo).values()\n",
        "    return pmv, ppd\n",
        "\n",
        "# Register the UDF\n",
        "pmv_ppd_udf = udf(calculate_pmv_ppd, StructType([\n",
        "    StructField(\"pmv\", FloatType(), True),\n",
        "    StructField(\"ppd\", FloatType(), True)\n",
        "]))\n",
        "\n",
        "# Access individual fields from the StructType column\n",
        "df = df.withColumn(\"pmv\", pmv_ppd_udf(df['tmp'], df['mrt'], df['v'], df['rhm'], df['met'], df['clo']))\n",
        "df = df.withColumn(\"ppd\", pmv_ppd_udf(df['tmp'], df['mrt'], df['v'], df['rhm'], df['met'], df['clo']))\n",
        "\n",
        "df = df.select('tmp', 'mrt', 'v', 'rhm', 'met', 'clo', 'pmv', 'ppd')\n",
        "\n",
        "\n",
        "print(df.count())\n",
        "df.take(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        },
        "id": "araShSpcTfxx",
        "outputId": "346aa833-2ee2-4727-fdd1-7d9e20e2b722"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3881524\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "PythonException",
          "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"<ipython-input-75-e75ef0d2cf88>\", line 9, in calculate_pmv_ppd\n  File \"/usr/local/lib/python3.10/dist-packages/pythermalcomfort/models/pmv_ppd.py\", line 177, in pmv_ppd\n    ) = check_standard_compliance_array(\n  File \"/usr/local/lib/python3.10/dist-packages/pythermalcomfort/utilities.py\", line 173, in check_standard_compliance_array\n    tdb_valid = valid_range(params[\"tdb\"], (10.0, 30.0))\n  File \"/usr/local/lib/python3.10/dist-packages/pythermalcomfort/shared_functions.py\", line 6, in valid_range\n    return np.where((x >= valid[0]) & (x <= valid[1]), x, np.nan)\nnumpy.core._exceptions._UFuncNoLoopError: ufunc 'greater_equal' did not contain a loop with signature matching types (<class 'numpy.dtypes.StrDType'>, <class 'numpy.dtypes.Float64DType'>) -> None\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-75-e75ef0d2cf88>\u001b[0m in \u001b[0;36m<cell line: 29>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# df.take(10)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1403\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Tom'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m23\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Alice'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m         \"\"\"\n\u001b[0;32m-> 1405\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1407\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \"\"\"\n\u001b[1;32m   1260\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1261\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1262\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"<ipython-input-75-e75ef0d2cf88>\", line 9, in calculate_pmv_ppd\n  File \"/usr/local/lib/python3.10/dist-packages/pythermalcomfort/models/pmv_ppd.py\", line 177, in pmv_ppd\n    ) = check_standard_compliance_array(\n  File \"/usr/local/lib/python3.10/dist-packages/pythermalcomfort/utilities.py\", line 173, in check_standard_compliance_array\n    tdb_valid = valid_range(params[\"tdb\"], (10.0, 30.0))\n  File \"/usr/local/lib/python3.10/dist-packages/pythermalcomfort/shared_functions.py\", line 6, in valid_range\n    return np.where((x >= valid[0]) & (x <= valid[1]), x, np.nan)\nnumpy.core._exceptions._UFuncNoLoopError: ufunc 'greater_equal' did not contain a loop with signature matching types (<class 'numpy.dtypes.StrDType'>, <class 'numpy.dtypes.Float64DType'>) -> None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"PMV_PPD_Calculator\").getOrCreate()\n",
        "\n",
        "\n",
        "\n",
        "df.take(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 741
        },
        "id": "43tR5W5Amxkj",
        "outputId": "f1a9383b-2dd0-49c0-df63-b538d84a0996"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "PythonException",
          "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 225, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 146, in dump_stream\n    for obj in iterator:\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 214, in _batched\n    for item in iterator:\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1070, in mapper\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1070, in <genexpr>\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 104, in <lambda>\n    return lambda *a: toInternal(f(*a))\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\nTypeError: calculate_pmv_ppd() missing 3 required positional arguments: 'rh', 'met', and 'clo'\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-1ea9350d2392>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# df = df.withColumn(\"clo\", lit(0.5))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1403\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Tom'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m23\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Alice'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m         \"\"\"\n\u001b[0;32m-> 1405\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1407\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \"\"\"\n\u001b[1;32m   1260\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1261\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1262\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 225, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 146, in dump_stream\n    for obj in iterator:\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 214, in _batched\n    for item in iterator:\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1070, in mapper\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1070, in <genexpr>\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 104, in <lambda>\n    return lambda *a: toInternal(f(*a))\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\nTypeError: calculate_pmv_ppd() missing 3 required positional arguments: 'rh', 'met', and 'clo'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yZ-QP4-72nP-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}